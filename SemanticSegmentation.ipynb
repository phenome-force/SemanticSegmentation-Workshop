{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # numpy arrays\n",
    "import pandas as pd # dataframe\n",
    "import seaborn as sns # plotting\n",
    "import matplotlib.pyplot as plt # plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pixel data collection for training finished on zooniverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](notebook_imgs/1.zooniverse.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "features  = np.load('Training_data/features.npy')/255 # normalize pixel value from 0-255 to 0-1\n",
    "print(labels.shape)\n",
    "\n",
    "labels = np.load('Training_data/labels.npy') # background, leaf, stalk , and panicle were marked by 0, 1, 2, and 3 respectively.\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize hyperspectral signatures of different plant parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preprocess data for plotting\n",
    "- use matplotlib and seaborn - popular Data Visualization libraries in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array to pandas dataframe and normalize reflectance from 0-255 to 0-1\n",
    "df = pd.DataFrame(features)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wavelength info. each wavelength respond to 1 band\n",
    "df_wave = pd.read_csv('wavelength_band_info.txt', delim_whitespace=True)\n",
    "df_wave.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add label data to the dataframe and rename each column in the dataframe\n",
    "df.columns = df_wave['Wavelength(nm)']\n",
    "df['Label'] = labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe transformation to match the input data format of lineplot function in seaborn \n",
    "# details about lineplot https://seaborn.pydata.org/generated/seaborn.lineplot.html\n",
    "df_melt = df.melt(id_vars='Label', value_name='Reflectance')\n",
    "df_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is what the [melt operation](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) looks like:\n",
    "![melt operation](notebook_imgs/melt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the data is ready, Let's start plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the colormap mapping 0,1,2,3 to different colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(['#BBBBBB', '#32CD32', '#FF8C00', '#9400D3'], name='organs') # hex code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can choose your color scheme in [colorbrew](https://colorbrewer2.org/#type=qualitative&scheme=Set1&n=4)\n",
    "![color scheme](notebook_imgs/color_scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 2.5)) # define a figure and a single axes object\n",
    "ax = sns.lineplot(ax=ax, data=df_melt, x='Wavelength(nm)', y='Reflectance',  \n",
    "                  hue='Label', ci=\"sd\", palette=cmap, linewidth=1)\n",
    "ax.set_xlim(530, 1711)\n",
    "ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8])\n",
    "ax.set_ylabel('Normalized Intensity', fontsize=8)\n",
    "ax.set_xlabel('Wavelength(nm)', fontsize=8)\n",
    "ax.legend(frameon=False, fontsize=7, labels=['background','leaf','stalk','panicle'])\n",
    "\n",
    "ax.spines['right'].set_visible(False) # remove right axis\n",
    "ax.spines['top'].set_visible(False) # remove top axis\n",
    "plt.tight_layout()\n",
    "plt.savefig('hyp_signature.png', dpi=300) # save figuer with resolution of 300 dpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images on cyverse](notebook_imgs/hyp_signature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal component analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unsupervised\n",
    "- exploratory data analysis\n",
    "- dimensionality reduction\n",
    "- more details about [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) on scikit-learn website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # import PCA class from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) # let's only consider the first two PCs\n",
    "pca_results = pca.fit_transform(features) # Fit the model with input and apply the dimensionality reduction on it.\n",
    "print(pca_results.shape)\n",
    "pca_scores = pca.explained_variance_ratio_ # variance explained by each component\n",
    "print('variance explained by the first two PCs: ', pca_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "s1, s2 = pca_results[:, 0], pca_results[:, 1]\n",
    "scatter = ax.scatter(s1, s2, c=labels, cmap=cmap, s=20, alpha=0.7)\n",
    "hs, _ = scatter.legend_elements() # handlers of the legend\n",
    "ls = ['background','leaf','stalk','panicle'] # labels of the legend\n",
    "ax.legend(hs, ls)\n",
    "ax.set_xlabel('PC1 (%.2f)'%pca_scores[0])\n",
    "ax.set_ylabel('PC2 (%.2f)'%pca_scores[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images on cyverse](notebook_imgs/pca.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold # for five fold cross validation\n",
    "from sklearn.metrics import confusion_matrix # calcuate confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (linear discriminant analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- supervised classifier\n",
    "- learn more details about [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) on scikit-learn website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features[train_index], features[test_index] # data for training\n",
    "    y_train, y_test = labels[train_index], labels[test_index] # data for validation\n",
    "    \n",
    "    clf = LDA() # you can specify 'n_components' option for dimensionality reduction like PCA\n",
    "    clf.fit(X_train, y_train) # fit the model with both features and labels because LDA is supervised approach\n",
    "    y_predict = clf.predict(X_test) # make predictions using fitted model\n",
    "    acc = (y_predict==y_test).sum()/len(y_test) # calcuate the overall accuracy\n",
    "    print('overal accuracy: %.3f'%acc)\n",
    "    cfx = confusion_matrix(y_test, y_predict, labels=[0,1,2,3], normalize='true') # check the confusion matrix. x-axis is ground truth and y-axis is predicted\n",
    "    print('accuracy in each class: ',np.diagonal(cfx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (support vector machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learn more details about [SVM](https://scikit-learn.org/stable/modules/svm.html) on scikit-learn website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear') # can choose different kernels\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    acc = (y_predict==y_test).sum()/len(y_test)\n",
    "    print('overal accuracy: %.3f'%acc)\n",
    "    cfx = confusion_matrix(y_test, y_predict, labels=[0,1,2,3], normalize='true')\n",
    "    print('accuracy in each class: ',np.diagonal(cfx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (k-nearest neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learn more details about [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on scikit-learn website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    clf = KNN(n_neighbors=3) # default is 5\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    acc = (y_predict==y_test).sum()/len(y_test)\n",
    "    print('overal accuracy: %.3f'%acc)\n",
    "    cfx = confusion_matrix(y_test, y_predict, labels=[0,1,2,3], normalize='true')\n",
    "    print('accuracy in each class: ',np.diagonal(cfx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learn more details about [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) on scikit-learn website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ginis = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    clf = RF() # try parameters: n_estimators=200, max_depth=5\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    acc = (y_predict==y_test).sum()/len(y_test)\n",
    "    print('overal accuracy: %.3f'%acc)\n",
    "    cfx = confusion_matrix(y_test, y_predict, labels=[0,1,2,3], normalize='true')\n",
    "    print('accuracy in each class: ',np.diagonal(cfx))\n",
    "    \n",
    "    gini = clf.feature_importances_ # Gini importance. The higher, the more important the feature.\n",
    "    ginis.append(gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gini = pd.DataFrame(ginis).transpose()\n",
    "df_gini.columns = ['fold %s'%i for i in range(1,6)]\n",
    "df_gini.index=df_wave['Wavelength(nm)']\n",
    "df_gini.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot gini importance across wavelengths\n",
    "ax = df_gini.plot()\n",
    "ax.set_ylabel('Gini importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('RF_Gini.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images on cyverse](notebook_imgs/RF_Gini.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Native Bayes and AdaBoost classifiers yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Native Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on a hyperspectral image cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seems LAD performs best among all tested classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model using all the training data\n",
    "clf = LDA()\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443, 320, 243)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the hyperspectral image cube in numpy array\n",
    "img_npy = np.load('NPYs/CM024_2017-08-30.npy')[35:478, :, :]/255 # remove the pot and top frame parts\n",
    "img_npy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remember that the input dimension for our model is (N, 243) where N is the number of pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "x, y, z = img_npy.shape\n",
    "x_test = img_npy.reshape(x*y, z)\n",
    "y_test = clf.predict(x_test).reshape(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show our prediction results\n",
    "cmap = ListedColormap(['#FFFFFF', '#32CD32', '#FF8C00', '#9400D3'], name='test')\n",
    "plt.imshow(y_test, cmap=cmap)\n",
    "# plt.imsave('test.png', y_test, cmap=cmap) # run if you want to save the prediction as a png file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original vs predicted:\n",
    "![images on cyverse](notebook_imgs/test_prediction1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to make predictions on other two hyperspectral image cubes under the 'NPYs' directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images on cyverse](notebook_imgs/test_prediction2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estiamte organ sizes from the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- other traits requiring image processing skills will not be covered here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_leaf = (y_test==1).sum() \n",
    "size_stalk = (y_test==2).sum()\n",
    "size_panicle = (y_test==3).sum()\n",
    "size_leaf, size_stalk, size_panicle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperspectral data in numpy format and Genotype data in hmp format for a sorghum diverse population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download all image data from [Cyverse](https://datacommons.cyverse.org/browse/iplant/home/shared/commons_repo/curated/Miao_Schnable_sorghumHighThroughputPhenotyping_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images on cyverse](notebook_imgs/cyverse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download genotype data from [figshare](https://doi.org/10.6084/m9.figshare.11462469.v5)\n",
    "![figshare genotype](notebook_imgs/figshare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To cite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite this paper if you use the hyperspectral image data for the sorghum association panel:\n",
    "- Chenyong Miao, Alejandro Pages, Zheng Xu, Eric Rodene, Jinliang Yang, and James C. Schnable (2020) Semantic segmentation of sorghum using hyperspectral data identifies genetic associations. **Plant Phenomics** doi: 10.34133/2020/4216373\n",
    "\n",
    "Please cite this paper if you use the genotype data for the same population\n",
    "- Chenyong Miao, Yuhang Xu, Sanzhen Liu, Patrick S. Schnable, and James C. Schnable (2020) Increased power and accuracy of locus identification in time-series genome-wide association in sorghum. **Plant Physiology** doi: 10.1101/2020.02.16.951467"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
